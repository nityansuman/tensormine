{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.331538Z",
     "start_time": "2019-07-16T10:10:25.743211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 The TensorHub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorhub.models.text.classifiers import PerceptronClassifier, RNNClassifier\n",
    "from tensorhub.utilities.processor import create_vocabulary, load_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Text Classification on 'News Healines' Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.353661Z",
     "start_time": "2019-07-16T10:10:27.334161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (100, 6)\n",
      "Columns: Index(['authors', 'category', 'date', 'headline', 'link', 'short_description'], dtype='object')\n",
      "Number of train samples: 75\n",
      "Number of test samples: 25\n"
     ]
    }
   ],
   "source": [
    "# Path to dataset in use\n",
    "filepath = \"samples_datasets/news_category.json\"\n",
    "\n",
    "# Load data into a dataframe\n",
    "df = pd.read_json(filepath, orient=\"records\", encoding=\"utf-8\", lines=True)\n",
    "\n",
    "print(\"Data Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Select feature and target column\n",
    "x = list(df.headline)\n",
    "y = list(df.category)\n",
    "\n",
    "# Split data into train and test\n",
    "# Train -> 75%\n",
    "# Test -> 25%\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "print(\"Number of train samples:\", len(x_train)) # List\n",
    "print(\"Number of test samples:\", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.360448Z",
     "start_time": "2019-07-16T10:10:27.355946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of word in a sequence are 11\n",
      "Maximum number of chars in a sequence are 76\n"
     ]
    }
   ],
   "source": [
    "# Compute the maximum number of words and characters in a sequence from the whole corpus\n",
    "max_num_words = len(max(x).split())\n",
    "max_num_chars = len(max(x))\n",
    "\n",
    "print(\"Maximum number of word in a sequence are\", max_num_words)\n",
    "print(\"Maximum number of chars in a sequence are\", max_num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.370363Z",
     "start_time": "2019-07-16T10:10:27.362777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a custom tokenizer on the corpus and generate tokenizer instance with vocabulary\n",
    "# type_embedding: 'word' & 'char'\n",
    "tokenizer, word_index = create_vocabulary(x, type_embedding=\"word\")\n",
    "\n",
    "# Or\n",
    "# Load pre-trained embedding matrix if learn_embedding is False. See model configuration.\n",
    "# embedding_matrix = Embeddings.load_embedding(word_index, filepath=\"PATH_TO_PRE_TRAINED_EMBEDDINGS\", dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.399808Z",
     "start_time": "2019-07-16T10:10:27.374321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " 'to': 2,\n",
       " 'trump': 3,\n",
       " 'the': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'a': 7,\n",
       " 'of': 8,\n",
       " 'and': 9,\n",
       " 'on': 10,\n",
       " 'with': 11,\n",
       " 'new': 12,\n",
       " 'is': 13,\n",
       " 'after': 14,\n",
       " 'donald': 15,\n",
       " 'his': 16,\n",
       " 'her': 17,\n",
       " \"trump's\": 18,\n",
       " 'abortion': 19,\n",
       " 'by': 20,\n",
       " 'over': 21,\n",
       " 'twitter': 22,\n",
       " 'it': 23,\n",
       " 'sexual': 24,\n",
       " 'harassment': 25,\n",
       " 'what': 26,\n",
       " 'reveals': 27,\n",
       " 'texas': 28,\n",
       " 'school': 29,\n",
       " 'more': 30,\n",
       " 'report': 31,\n",
       " 'harvey': 32,\n",
       " 'weinstein': 33,\n",
       " 'kim': 34,\n",
       " 'wins': 35,\n",
       " 'adam': 36,\n",
       " 'up': 37,\n",
       " 'this': 38,\n",
       " 'shooting': 39,\n",
       " 'parents': 40,\n",
       " 'kids': 41,\n",
       " 'be': 42,\n",
       " 'fbi': 43,\n",
       " 'one': 44,\n",
       " 'back': 45,\n",
       " 'men': 46,\n",
       " 'you': 47,\n",
       " 'emilia': 48,\n",
       " 'clarke': 49,\n",
       " 'alleged': 50,\n",
       " 'summit': 51,\n",
       " 'party': 52,\n",
       " 'abuse': 53,\n",
       " 'kentucky': 54,\n",
       " 'primary': 55,\n",
       " 'house': 56,\n",
       " 'democratic': 57,\n",
       " 'congressional': 58,\n",
       " 'world': 59,\n",
       " 'at': 60,\n",
       " 'pick': 61,\n",
       " 'that': 62,\n",
       " \"'tonight\": 63,\n",
       " \"show'\": 64,\n",
       " 'watch': 65,\n",
       " 'that’s': 66,\n",
       " 'week': 67,\n",
       " 'do': 68,\n",
       " 'film': 69,\n",
       " 'victims': 70,\n",
       " 'immigrant': 71,\n",
       " 'should': 72,\n",
       " 'putin': 73,\n",
       " 'who': 74,\n",
       " 'than': 75,\n",
       " 'ryan': 76,\n",
       " 'looks': 77,\n",
       " 'critics': 78,\n",
       " 'people': 79,\n",
       " 'are': 80,\n",
       " 'like': 81,\n",
       " 'again': 82,\n",
       " 'sue': 83,\n",
       " 'about': 84,\n",
       " 'says': 85,\n",
       " \"doesn't\": 86,\n",
       " 'get': 87,\n",
       " 'make': 88,\n",
       " 'never': 89,\n",
       " 'himself': 90,\n",
       " 'sex': 91,\n",
       " 'crimes': 92,\n",
       " 'now': 93,\n",
       " 'own': 94,\n",
       " 'when': 95,\n",
       " \"won't\": 96,\n",
       " 'try': 97,\n",
       " 'tomi': 98,\n",
       " 'lahren': 99,\n",
       " 'while': 100,\n",
       " 'north': 101,\n",
       " 'korea': 102,\n",
       " 'jong': 103,\n",
       " 'un': 104,\n",
       " 'children': 105,\n",
       " 'shows': 106,\n",
       " 'senate': 107,\n",
       " 'judge': 108,\n",
       " 'blocking': 109,\n",
       " 'ellen': 110,\n",
       " 'ariana': 111,\n",
       " 'grande': 112,\n",
       " 'pretty': 113,\n",
       " 'farrow': 114,\n",
       " 'brad': 115,\n",
       " 'pitt': 116,\n",
       " 'jessica': 117,\n",
       " 'walter': 118,\n",
       " 'jeffrey': 119,\n",
       " 'introduced': 120,\n",
       " 'canceling': 121,\n",
       " 'undocumented': 122,\n",
       " 'from': 123,\n",
       " 'jones': 124,\n",
       " 'gop': 125,\n",
       " 'netflix': 126,\n",
       " 'restaurant': 127,\n",
       " 'down': 128,\n",
       " 'as': 129,\n",
       " 'sarah': 130,\n",
       " 'runoff': 131,\n",
       " 'gun': 132,\n",
       " \"stars'\": 133,\n",
       " 'will': 134,\n",
       " 'smith': 135,\n",
       " 'joins': 136,\n",
       " 'diplo': 137,\n",
       " 'nicky': 138,\n",
       " 'jam': 139,\n",
       " '2018': 140,\n",
       " \"cup's\": 141,\n",
       " 'official': 142,\n",
       " 'song': 143,\n",
       " 'hugh': 144,\n",
       " 'grant': 145,\n",
       " 'marries': 146,\n",
       " 'first': 147,\n",
       " 'time': 148,\n",
       " 'age': 149,\n",
       " '57': 150,\n",
       " 'jim': 151,\n",
       " 'carrey': 152,\n",
       " 'blasts': 153,\n",
       " \"'castrato'\": 154,\n",
       " 'schiff': 155,\n",
       " 'democrats': 156,\n",
       " 'artwork': 157,\n",
       " 'julianna': 158,\n",
       " 'margulies': 159,\n",
       " 'uses': 160,\n",
       " 'poop': 161,\n",
       " 'bags': 162,\n",
       " 'dog': 163,\n",
       " 'morgan': 164,\n",
       " 'freeman': 165,\n",
       " \"'devastated'\": 166,\n",
       " 'claims': 167,\n",
       " 'could': 168,\n",
       " 'undermine': 169,\n",
       " 'legacy': 170,\n",
       " \"lovin'\": 171,\n",
       " \"mcdonald's\": 172,\n",
       " 'jingle': 173,\n",
       " 'bit': 174,\n",
       " 'amazon': 175,\n",
       " 'prime': 176,\n",
       " 'mike': 177,\n",
       " 'myers': 178,\n",
       " \"he'd\": 179,\n",
       " \"'like\": 180,\n",
       " \"to'\": 181,\n",
       " 'fourth': 182,\n",
       " 'austin': 183,\n",
       " 'powers': 184,\n",
       " 'hulu': 185,\n",
       " 'justin': 186,\n",
       " 'timberlake': 187,\n",
       " 'visits': 188,\n",
       " 'crackdown': 189,\n",
       " 'puts': 190,\n",
       " 'an': 191,\n",
       " 'already': 192,\n",
       " 'strained': 193,\n",
       " 'system': 194,\n",
       " \"'trump's\": 195,\n",
       " 'son': 196,\n",
       " \"concerned'\": 197,\n",
       " 'obtained': 198,\n",
       " 'wiretaps': 199,\n",
       " 'ally': 200,\n",
       " 'met': 201,\n",
       " 'jr': 202,\n",
       " 'edward': 203,\n",
       " 'snowden': 204,\n",
       " \"there's\": 205,\n",
       " 'no': 206,\n",
       " 'loves': 207,\n",
       " 'vladimir': 208,\n",
       " 'booyah': 209,\n",
       " 'obama': 210,\n",
       " 'photographer': 211,\n",
       " 'hilariously': 212,\n",
       " 'trolls': 213,\n",
       " \"'spy'\": 214,\n",
       " 'claim': 215,\n",
       " 'ireland': 216,\n",
       " 'votes': 217,\n",
       " 'repeal': 218,\n",
       " 'amendment': 219,\n",
       " 'landslide': 220,\n",
       " 'referendum': 221,\n",
       " 'zinke': 222,\n",
       " 'reel': 223,\n",
       " 'some': 224,\n",
       " \"'grand\": 225,\n",
       " \"pivot'\": 226,\n",
       " 'conservation': 227,\n",
       " 'scottish': 228,\n",
       " 'golf': 229,\n",
       " 'resort': 230,\n",
       " 'pays': 231,\n",
       " 'women': 232,\n",
       " 'significantly': 233,\n",
       " 'less': 234,\n",
       " 'rattled': 235,\n",
       " 'how': 236,\n",
       " 'much': 237,\n",
       " 'nigerian': 238,\n",
       " 'man': 239,\n",
       " 'chadwick': 240,\n",
       " 'boseman': 241,\n",
       " 'david': 242,\n",
       " 'cross': 243,\n",
       " 'proves': 244,\n",
       " 'yet': 245,\n",
       " 'he’s': 246,\n",
       " 'terrible': 247,\n",
       " 'apologizing': 248,\n",
       " \"'sesame\": 249,\n",
       " \"street'\": 250,\n",
       " 'creators': 251,\n",
       " 'makers': 252,\n",
       " 'upcoming': 253,\n",
       " 'adult': 254,\n",
       " 'themed': 255,\n",
       " 'puppet': 256,\n",
       " 'missed': 257,\n",
       " 'saddest': 258,\n",
       " 'death': 259,\n",
       " \"'avengers\": 260,\n",
       " 'infinity': 261,\n",
       " \"war'\": 262,\n",
       " 'george': 263,\n",
       " 'takei': 264,\n",
       " 'accuser': 265,\n",
       " 'walks': 266,\n",
       " 'story': 267,\n",
       " 'drugging': 268,\n",
       " 'assault': 269,\n",
       " 'samantha': 270,\n",
       " \"bee's\": 271,\n",
       " 'think': 272,\n",
       " 'america': 273,\n",
       " 'basically': 274,\n",
       " 'war': 275,\n",
       " 'zone': 276,\n",
       " 'pete': 277,\n",
       " 'davidson': 278,\n",
       " 'mental': 279,\n",
       " 'illness': 280,\n",
       " 'ruin': 281,\n",
       " 'romance': 282,\n",
       " 'james': 283,\n",
       " 'corden': 284,\n",
       " 'levine': 285,\n",
       " 'pulled': 286,\n",
       " 'during': 287,\n",
       " \"'carpool\": 288,\n",
       " \"karaoke'\": 289,\n",
       " 'dishes': 290,\n",
       " 'final': 291,\n",
       " \"'game\": 292,\n",
       " \"thrones'\": 293,\n",
       " 'scene': 294,\n",
       " 'hollywood': 295,\n",
       " 'need': 296,\n",
       " \"'difficult'\": 297,\n",
       " 'great': 298,\n",
       " 'tv': 299,\n",
       " 'accusers': 300,\n",
       " 'say': 301,\n",
       " 'they': 302,\n",
       " 'thought': 303,\n",
       " 'he': 304,\n",
       " 'would': 305,\n",
       " 'arrested': 306,\n",
       " 'turns': 307,\n",
       " 'police': 308,\n",
       " 'john': 309,\n",
       " \"mayer's\": 310,\n",
       " 'weird': 311,\n",
       " 'low': 312,\n",
       " 'budget': 313,\n",
       " 'video': 314,\n",
       " 'meme': 315,\n",
       " 'worthy': 316,\n",
       " 'masterpiece': 317,\n",
       " 'boba': 318,\n",
       " 'fett': 319,\n",
       " 'getting': 320,\n",
       " \"'star\": 321,\n",
       " \"wars'\": 322,\n",
       " 'movie': 323,\n",
       " 'member': 324,\n",
       " 'far': 325,\n",
       " 'right': 326,\n",
       " 'proud': 327,\n",
       " 'boys': 328,\n",
       " 'menaced': 329,\n",
       " 'user': 330,\n",
       " 'doorstep': 331,\n",
       " 'all': 332,\n",
       " 'things': 333,\n",
       " 'nra': 334,\n",
       " 'has': 335,\n",
       " 'blamed': 336,\n",
       " \"here's\": 337,\n",
       " 'happens': 338,\n",
       " 'catholic': 339,\n",
       " 'hospital': 340,\n",
       " 'save': 341,\n",
       " 'executive': 342,\n",
       " 'orders': 343,\n",
       " 'easier': 344,\n",
       " 'fire': 345,\n",
       " 'federal': 346,\n",
       " 'workers': 347,\n",
       " 'roasted': 348,\n",
       " 'rushing': 349,\n",
       " 'defend': 350,\n",
       " 'ignoring': 351,\n",
       " 'real': 352,\n",
       " 'abruptly': 353,\n",
       " 'cancels': 354,\n",
       " 'devin': 355,\n",
       " 'nunes': 356,\n",
       " 'linked': 357,\n",
       " 'winery': 358,\n",
       " 'faced': 359,\n",
       " 'suit': 360,\n",
       " 'coke': 361,\n",
       " 'prostitute': 362,\n",
       " 'yacht': 363,\n",
       " 'accuse': 364,\n",
       " 'border': 365,\n",
       " 'patrol': 366,\n",
       " 'neglect': 367,\n",
       " \"ireland's\": 368,\n",
       " 'historic': 369,\n",
       " 'vote': 370,\n",
       " 'legalizing': 371,\n",
       " 'haunted': 372,\n",
       " 'brexit': 373,\n",
       " 'lawyer': 374,\n",
       " 'attended': 375,\n",
       " 'doj': 376,\n",
       " 'meeting': 377,\n",
       " 'confidential': 378,\n",
       " 'informant': 379,\n",
       " 'advances': 380,\n",
       " 'court': 381,\n",
       " 'said': 382,\n",
       " 'planned': 383,\n",
       " 'parenthood': 384,\n",
       " \"'kills\": 385,\n",
       " '150': 386,\n",
       " '000': 387,\n",
       " 'females': 388,\n",
       " 'year’': 389,\n",
       " 'rudy': 390,\n",
       " 'giuliani': 391,\n",
       " 'interview': 392,\n",
       " 'mueller': 393,\n",
       " 'until': 394,\n",
       " 'we': 395,\n",
       " '‘spygate’': 396,\n",
       " 'attorney': 397,\n",
       " 'running': 398,\n",
       " 'dies': 399,\n",
       " '1': 400,\n",
       " 'day': 401,\n",
       " 'win': 402,\n",
       " 'chrissy': 403,\n",
       " 'teigen': 404,\n",
       " 'taunts': 405,\n",
       " 'ruling': 406,\n",
       " 'koch': 407,\n",
       " 'network': 408,\n",
       " 'going': 409,\n",
       " 'trump’s': 410,\n",
       " 'favorite': 411,\n",
       " 'congressmen': 412,\n",
       " 'tears': 413,\n",
       " 'ashton': 414,\n",
       " 'kutcher': 415,\n",
       " 'donates': 416,\n",
       " '4': 417,\n",
       " 'million': 418,\n",
       " 'wildlife': 419,\n",
       " 'fund': 420,\n",
       " 'bee': 421,\n",
       " 'tattoo': 422,\n",
       " 'honor': 423,\n",
       " 'manchester': 424,\n",
       " 'attack': 425,\n",
       " 'kardashian': 426,\n",
       " 'wished': 427,\n",
       " 'kanye': 428,\n",
       " 'west': 429,\n",
       " 'happy': 430,\n",
       " 'anniversary': 431,\n",
       " 'fine': 432,\n",
       " \"it's\": 433,\n",
       " 'cute': 434,\n",
       " 'chris': 435,\n",
       " 'hemsworth': 436,\n",
       " 'dances': 437,\n",
       " \"'wrecking\": 438,\n",
       " \"ball'\": 439,\n",
       " 'living': 440,\n",
       " 'room': 441,\n",
       " 'moses': 442,\n",
       " 'defends': 443,\n",
       " 'woody': 444,\n",
       " 'allen': 445,\n",
       " 'accuses': 446,\n",
       " 'mia': 447,\n",
       " 'gwyneth': 448,\n",
       " 'paltrow': 449,\n",
       " 'threatened': 450,\n",
       " 'kill': 451,\n",
       " 'stephen': 452,\n",
       " 'king': 453,\n",
       " 'why': 454,\n",
       " 'blocked': 455,\n",
       " 'him': 456,\n",
       " 'fans': 457,\n",
       " 'she': 458,\n",
       " 'addressed': 459,\n",
       " 'verbal': 460,\n",
       " 'tambor': 461,\n",
       " 'discusses': 462,\n",
       " \"tambor's\": 463,\n",
       " 'male': 464,\n",
       " 'co': 465,\n",
       " 'stars': 466,\n",
       " 'brush': 467,\n",
       " 'off': 468,\n",
       " 'expected': 469,\n",
       " 'turn': 470,\n",
       " 'nypd': 471,\n",
       " 'scott': 472,\n",
       " 'pruitt': 473,\n",
       " 'twice': 474,\n",
       " 'anti': 475,\n",
       " 'bills': 476,\n",
       " 'giving': 477,\n",
       " '‘property': 478,\n",
       " 'rights’': 479,\n",
       " 'fetuses': 480,\n",
       " 'mercilessly': 481,\n",
       " 'mock': 482,\n",
       " 'civil': 483,\n",
       " 'rights': 484,\n",
       " 'groups': 485,\n",
       " 'rip': 486,\n",
       " 'bill': 487,\n",
       " 'revising': 488,\n",
       " 'its': 489,\n",
       " 'policy': 490,\n",
       " 'alabama': 491,\n",
       " 'sues': 492,\n",
       " 'exclude': 493,\n",
       " 'immigrants': 494,\n",
       " 'census': 495,\n",
       " 'count': 496,\n",
       " 'cynthia': 497,\n",
       " 'nixon': 498,\n",
       " 'vows': 499,\n",
       " 'keep': 500,\n",
       " 'fighting': 501,\n",
       " 'predictably': 502,\n",
       " 'losing': 503,\n",
       " 'endorsement': 504,\n",
       " 'white': 505,\n",
       " 'officials': 506,\n",
       " 'reportedly': 507,\n",
       " 'considered': 508,\n",
       " 'just': 509,\n",
       " \"'ignoring'\": 510,\n",
       " 'climate': 511,\n",
       " 'research': 512,\n",
       " 'read': 513,\n",
       " 'letter': 514,\n",
       " 'betsy': 515,\n",
       " 'devos': 516,\n",
       " 'stirs': 517,\n",
       " 'uproar': 518,\n",
       " 'saying': 519,\n",
       " 'schools': 520,\n",
       " 'can': 521,\n",
       " 'call': 522,\n",
       " 'ice': 523,\n",
       " '6': 524,\n",
       " 'sandy': 525,\n",
       " 'hook': 526,\n",
       " 'families': 527,\n",
       " 'agent': 528,\n",
       " 'alex': 529,\n",
       " 'defamation': 530,\n",
       " 'high': 531,\n",
       " 'teacher': 532,\n",
       " 'defeats': 533,\n",
       " 'majority': 534,\n",
       " 'leader': 535,\n",
       " 'lupe': 536,\n",
       " 'valdez': 537,\n",
       " 'makes': 538,\n",
       " 'history': 539,\n",
       " 'winning': 540,\n",
       " 'nod': 541,\n",
       " 'governor': 542,\n",
       " 'michael': 543,\n",
       " \"cohen's\": 544,\n",
       " 'business': 545,\n",
       " 'partner': 546,\n",
       " 'cooperating': 547,\n",
       " 'prosecutors': 548,\n",
       " '‘13': 549,\n",
       " 'reasons': 550,\n",
       " 'why’': 551,\n",
       " 'started': 552,\n",
       " 'conversation': 553,\n",
       " 'who’s': 554,\n",
       " 'responsible': 555,\n",
       " 'finishing': 556,\n",
       " 'lourdes': 557,\n",
       " 'leon': 558,\n",
       " 'gives': 559,\n",
       " 'us': 560,\n",
       " 'major': 561,\n",
       " 'madonna': 562,\n",
       " 'vibes': 563,\n",
       " 'moody': 564,\n",
       " 'ad': 565,\n",
       " 'campaign': 566,\n",
       " 'pics': 567,\n",
       " '5': 568,\n",
       " 'ways': 569,\n",
       " 'tricks': 570,\n",
       " 'into': 571,\n",
       " 'watching': 572,\n",
       " 'movies': 573,\n",
       " 'made': 574,\n",
       " 'reynolds': 575,\n",
       " 'demeaning': 576,\n",
       " 'thing': 577,\n",
       " 'exchange': 578,\n",
       " \"'deadpool\": 579,\n",
       " \"2'\": 580,\n",
       " 'cameo': 581,\n",
       " 'robert': 582,\n",
       " 'de': 583,\n",
       " 'niro': 584,\n",
       " 'bans': 585,\n",
       " 'every': 586,\n",
       " 'nobu': 587,\n",
       " 'shuts': 588,\n",
       " 'fan': 589,\n",
       " 'blaming': 590,\n",
       " '‘heartbreaking’': 591,\n",
       " 'mac': 592,\n",
       " 'miller': 593,\n",
       " 'split': 594,\n",
       " 'slam': 595,\n",
       " \"'show\": 596,\n",
       " \"dogs'\": 597,\n",
       " 'scenes': 598,\n",
       " 'grooming': 599,\n",
       " 'dave': 600,\n",
       " 'grohl': 601,\n",
       " 'wants': 602,\n",
       " 'apologize': 603,\n",
       " '‘massive': 604,\n",
       " 'jerk’': 605,\n",
       " 'herself': 606,\n",
       " 'prince': 607,\n",
       " 'william': 608,\n",
       " 'embarrassing': 609,\n",
       " 'way': 610,\n",
       " 'paulson': 611,\n",
       " 'takes': 612,\n",
       " 'impressions': 613,\n",
       " 'next': 614,\n",
       " 'level': 615,\n",
       " 'b': 616,\n",
       " '52s': 617,\n",
       " 'still': 618,\n",
       " 'ready': 619,\n",
       " 'taken': 620,\n",
       " 'seriously': 621,\n",
       " 'gina': 622,\n",
       " 'ortiz': 623,\n",
       " 'race': 624,\n",
       " 'fighter': 625,\n",
       " 'pilot': 626,\n",
       " 'amy': 627,\n",
       " 'mcgrath': 628,\n",
       " 'swing': 629,\n",
       " 'district': 630,\n",
       " 'rallies': 631,\n",
       " 'conservatives': 632,\n",
       " 'promise': 633,\n",
       " 'curbs': 634,\n",
       " 'aclu': 635,\n",
       " 'suing': 636,\n",
       " 'strike': 637,\n",
       " \"ohio's\": 638,\n",
       " 'map': 639,\n",
       " 'latest': 640,\n",
       " \"hasn't\": 641,\n",
       " 'really': 642,\n",
       " 'stirred': 643,\n",
       " 'debate': 644,\n",
       " 'tweet': 645,\n",
       " 'rescues': 646,\n",
       " \"'roseanne'\": 647,\n",
       " 'family': 648,\n",
       " 'season': 649,\n",
       " 'finale': 650,\n",
       " 'grills': 651,\n",
       " 'mark': 652,\n",
       " 'wahlberg': 653,\n",
       " 'shirtless': 654,\n",
       " 'instagram': 655,\n",
       " 'selfies': 656,\n",
       " 'idris': 657,\n",
       " 'elba': 658,\n",
       " 'star': 659,\n",
       " 'hunchback': 660,\n",
       " 'notre': 661,\n",
       " 'dame': 662,\n",
       " 'praises': 663,\n",
       " 'crowd': 664,\n",
       " 'tosses': 665,\n",
       " 'water': 666,\n",
       " 'insults': 667,\n",
       " 'sanders': 668,\n",
       " \"'bothers\": 669,\n",
       " \"me'\": 670,\n",
       " 'being': 671,\n",
       " 'called': 672,\n",
       " 'liar': 673,\n",
       " 'unconstitutional': 674,\n",
       " 'rules': 675,\n",
       " 'mostly': 676,\n",
       " 'non': 677,\n",
       " 'christian': 678,\n",
       " 'militia': 679,\n",
       " 'won': 680,\n",
       " '2': 681,\n",
       " 'iraqi': 682,\n",
       " \"christians'\": 683,\n",
       " 'parliamentary': 684,\n",
       " 'seats': 685,\n",
       " 'christians': 686,\n",
       " 'want': 687,\n",
       " 'intervene': 688,\n",
       " 'tulsi': 689,\n",
       " \"gabbard's\": 690,\n",
       " 'syria': 691,\n",
       " 'views': 692,\n",
       " 'cost': 693,\n",
       " 'support': 694,\n",
       " 'hawaii': 695,\n",
       " 'teachers': 696,\n",
       " 'union': 697,\n",
       " 'jared': 698,\n",
       " 'kushner': 699,\n",
       " 'received': 700,\n",
       " 'security': 701,\n",
       " 'clearance': 702,\n",
       " 'reports': 703,\n",
       " 'arizona': 704,\n",
       " 'candidate': 705,\n",
       " 'compares': 706,\n",
       " 'holocaust': 707,\n",
       " 'reform': 708,\n",
       " 'advocate': 709,\n",
       " 'lucy': 710,\n",
       " 'mcbath': 711,\n",
       " 'heads': 712,\n",
       " 'georgia': 713,\n",
       " 'seat': 714,\n",
       " 'miley': 715,\n",
       " 'cyrus': 716,\n",
       " 'comes': 717,\n",
       " 'wrecking': 718,\n",
       " 'ball': 719,\n",
       " 'prank': 720,\n",
       " 'sleeping': 721,\n",
       " 'jimmy': 722,\n",
       " 'kimmel': 723,\n",
       " 'vanessa': 724,\n",
       " 'bayer': 725,\n",
       " 'feared': 726,\n",
       " 'doing': 727,\n",
       " \"'porn\": 728,\n",
       " 'skit': 729,\n",
       " \"'snl'\": 730,\n",
       " 'rippon': 731,\n",
       " \"'dancing\": 732,\n",
       " 'because': 733,\n",
       " 'was': 734,\n",
       " 'destined': 735,\n",
       " 'imitate': 736,\n",
       " 'chewbacca': 737,\n",
       " 'ever': 738,\n",
       " 'maddie': 739,\n",
       " 'poppe': 740,\n",
       " \"'american\": 741,\n",
       " \"idol'\": 742,\n",
       " 'runner': 743,\n",
       " \"they're\": 744,\n",
       " 'dating': 745}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.416497Z",
     "start_time": "2019-07-16T10:10:27.403364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1320804e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.429334Z",
     "start_time": "2019-07-16T10:10:27.420770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize data using the created tokenizer instance\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad or truncate sequences to make fixed length input\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train,\n",
    "    value=0, # Pad with 0\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\",\n",
    "    maxlen=max_num_words # When using 'char' embedding use max_num_chars\n",
    ")\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test,\n",
    "    value=0, # Pad with 0\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\",\n",
    "    maxlen=max_num_words # When using 'char' embedding use max_num_chars\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.435128Z",
     "start_time": "2019-07-16T10:10:27.432358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data type to float from int\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.442672Z",
     "start_time": "2019-07-16T10:10:27.438081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all class labels\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "# Generate a unique index for each class from 0 to len(classes) - 1\n",
    "index = range(len(classes))\n",
    "\n",
    "# Create class to index mapping\n",
    "class_index = dict(zip(classes, index))\n",
    "reverse_class_index = dict(zip(index, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.449854Z",
     "start_time": "2019-07-16T10:10:27.445649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now encode labels using the created mapping\n",
    "y_train = [class_index[label] for label in y_train]\n",
    "y_test = [class_index[label] for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.454981Z",
     "start_time": "2019-07-16T10:10:27.451980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to categorical\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=len(classes))\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:27.478558Z",
     "start_time": "2019-07-16T10:10:27.456726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create batch datasets: batches of 32 for train and 64 for test\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:29.214342Z",
     "start_time": "2019-07-16T10:10:27.482204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load one of our pre-cooked models\n",
    "# LSTM model\n",
    "custom_model = RNNClassifier(model_name=\"lstm\", vocab_size=len(word_index) + 1, num_classes=len(classes)).model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET MODEL TRAINING AND VALIDATION ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:10:29.226710Z",
     "start_time": "2019-07-16T10:10:29.216324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train model using a tensor function\n",
    "# In future, this will be separated as a different module with multiple development and production environment\n",
    "def train_validate_model(model, train_ds, test_ds, epochs=3):\n",
    "    # Define model configuration\n",
    "    loss_function = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = keras.optimizers.RMSprop()\n",
    "\n",
    "    # Accumulate performance metrics while training\n",
    "    train_loss = keras.metrics.Mean(name=\"train_loss\")\n",
    "    train_accuracy = keras.metrics.CategoricalAccuracy(name=\"train_accuracy\")\n",
    "    test_loss = keras.metrics.Mean(name=\"test_loss\")\n",
    "    test_accuracy = keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "    template = \"Epoch {}, Loss: {}, Accuracy: {}%, Test Loss: {}, Test Accuracy: {}%\"\n",
    "\n",
    "    @tf.function()\n",
    "    def train_step(text, labels):\n",
    "        # Use gradient tape for training the model\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get predictions\n",
    "            predictions = model(text)\n",
    "            # Compute instantaneous loss\n",
    "            loss = loss_function(labels, predictions)\n",
    "        # Update gradients\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        # Store\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)\n",
    "        \n",
    "    # Test model using another tensor function\n",
    "    @tf.function()\n",
    "    def test_step(text, labels):\n",
    "        # Get predictions\n",
    "        predictions = model(text)\n",
    "        # Compute instantaneous loss\n",
    "        loss = loss_function(labels, predictions)\n",
    "        # Store\n",
    "        test_loss(loss)\n",
    "        test_accuracy(labels, predictions)\n",
    "\n",
    "    print(\"{:#^50s}\".format(\"Train and Validate\"))\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "        \n",
    "        # Train model on batches\n",
    "        for text, labels in train_ds:\n",
    "            train_step(text, labels)\n",
    "        \n",
    "        # Test model on batches\n",
    "        for t_text, t_labels in test_ds:\n",
    "            test_step(t_text, t_labels)\n",
    "\n",
    "        # Prompt user\n",
    "        print(template.format(epoch, train_loss.result(), train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T10:12:07.075025Z",
     "start_time": "2019-07-16T10:11:57.249875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################Train and Validate################\n",
      "Epoch 1/3\n",
      "Epoch 1, Loss: 0.6607358455657959, Accuracy: 56.0%, Test Loss: 1.9638547897338867, Test Accuracy: 44.0%\n",
      "Epoch 2/3\n",
      "Epoch 2, Loss: 0.8429980278015137, Accuracy: 56.0%, Test Loss: 1.3557738065719604, Test Accuracy: 44.0%\n",
      "Epoch 3/3\n",
      "Epoch 3, Loss: 0.7486786246299744, Accuracy: 56.88888931274414%, Test Loss: 1.245554804801941, Test Accuracy: 44.0%\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "train_validate_model(custom_model, train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:package_test]",
   "language": "python",
   "name": "conda-env-package_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
